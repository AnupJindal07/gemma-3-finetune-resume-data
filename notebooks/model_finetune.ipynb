{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9be85e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "9c9be85e"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "import constants\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "model_name = constants.model_name\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use the EOS token as padding\n",
        "tokenizer.padding_side = \"right\"  # Ensure padding is on the right for causal LM\n",
        "\n",
        "# Apply 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "# Enable training mode\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Prepare model for k-bit training BEFORE applying LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "'''\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "'''\n",
        "config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",          # Causal language modeling\n",
        "    r=16,                                   # Rank of adaptation\n",
        "    lora_alpha=32,                         # LoRA scaling parameter\n",
        "    lora_dropout=0.1,                      # LoRA dropout\n",
        "    target_modules=[                       # Target modules for Gemma-3\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    bias=\"none\",                           # No bias training\n",
        "    inference_mode=False                   # Training mode\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# Print LoRA info to verify it's working\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": constants.process_file_path})\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    input_texts = examples[\"prompt\"]\n",
        "    output_texts = examples[\"response\"]\n",
        "\n",
        "    # Format conversations using Gemma-3 IT chat template\n",
        "    conversations = []\n",
        "    for prompt, response in zip(input_texts, output_texts):\n",
        "        # Create conversation in Gemma-3 IT format\n",
        "        conversation = [\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"assistant\", \"content\": response}\n",
        "        ]\n",
        "        conversations.append(conversation)\n",
        "\n",
        "    # Use the tokenizer's chat template to format the conversations\n",
        "    formatted_texts = []\n",
        "    for conversation in conversations:\n",
        "        # Apply chat template\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        formatted_texts.append(formatted_text)\n",
        "\n",
        "    # Tokenize the formatted conversations\n",
        "    tokenized = tokenizer(\n",
        "        formatted_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    # Create labels by masking the user prompt tokens\n",
        "    labels = []\n",
        "    for i, conversation in enumerate(conversations):\n",
        "        # Create the user part only to find where assistant response starts\n",
        "        user_conversation = [{\"role\": \"user\", \"content\": conversation[0][\"content\"]}]\n",
        "        user_text = tokenizer.apply_chat_template(\n",
        "            user_conversation,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True  # This adds the assistant prompt\n",
        "        )\n",
        "\n",
        "        # Tokenize user part to find the boundary\n",
        "        user_tokens = tokenizer(user_text, add_special_tokens=False)[\"input_ids\"]\n",
        "        full_tokens = tokenized[\"input_ids\"][i]\n",
        "\n",
        "        # Create label sequence - start with all masked\n",
        "        label_seq = [-100] * len(full_tokens)\n",
        "\n",
        "        # Only learn from the assistant response part\n",
        "        user_length = len(user_tokens)\n",
        "        if user_length < len(full_tokens):\n",
        "            # Copy the assistant response tokens to labels (unmask them)\n",
        "            for j in range(user_length, len(full_tokens)):\n",
        "                if full_tokens[j] != tokenizer.pad_token_id:  # Don't learn from padding\n",
        "                    label_seq[j] = full_tokens[j]\n",
        "\n",
        "        labels.append(label_seq)\n",
        "\n",
        "    # Debug: print first example\n",
        "    if len(labels) > 0:\n",
        "        non_masked_count = sum(1 for x in labels[0] if x != -100)\n",
        "        print(f\"Debug: First example has {non_masked_count} non-masked tokens to learn from\")\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the original text columns that are causing issues\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"prompt\", \"response\"])\n",
        "\n",
        "# Split into train and eval datasets (85-15 ratio)\n",
        "train_eval_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.20, seed=42)\n",
        "train_dataset = train_eval_split[\"train\"]\n",
        "eval_dataset = train_eval_split[\"test\"]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
        "batch_size = 2\n",
        "gradient_accumulation_steps = 32\n",
        "num_epochs = 3\n",
        "\n",
        "\n",
        "# Don't use DataCollatorForLanguageModeling as it interferes with our custom labels\n",
        "# Use a simple data collator that just handles padding\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=constants.model_checkpoint_path,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    num_train_epochs=num_epochs,\n",
        "    learning_rate=2e-4,  # Higher learning rate for LoRA\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=20,\n",
        "\n",
        "    logging_dir=\"logs\",\n",
        "    logging_steps=20,\n",
        "\n",
        "    eval_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    save_total_limit=4,\n",
        "    load_best_model_at_end=True,\n",
        "    max_grad_norm=1.0,   # Gradient clipping\n",
        "    #metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=False,                      # Use bfloat16 instead\n",
        "    bf16=True,                       # Better for modern GPUs\n",
        "    dataloader_pin_memory=False,     # Save memory, Reduce memory pressure\n",
        "    gradient_checkpointing=True,     # Trade compute for memory\n",
        "    remove_unused_columns=False,     # Keep all columns including labels\n",
        "    report_to=None,                  # Disable logging to avoid conflicts\n",
        "    run_name=f\"gemma-3-4b-resume-qlora-{batch_size}bs-{num_epochs}ep\",\n",
        "    seed=42,\n",
        "    #torch_compile=False,             # Disable torch compile for compatibility\n",
        "    #dataloader_num_workers=0,        # Single-threaded data loading for stability\n",
        ")\n",
        "\n",
        "from transformers import EarlyStoppingCallback\n",
        "# Configure a callback to stop training if the evaluation loss\n",
        "# doesn't improve for 3 consecutive evaluations.\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    #tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55005374",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "55005374"
      },
      "outputs": [],
      "source": [
        "print(f\"Started training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "trainer.train()\n",
        "model.save_pretrained(constants.trained_model_path)\n",
        "tokenizer.save_pretrained(constants.trained_model_path)\n",
        "print(f\"Completed training at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}